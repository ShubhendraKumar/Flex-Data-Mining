{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import chromedriver_binary\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.expected_conditions import NoAlertPresentException\n",
    "from selenium.webdriver.support.expected_conditions import NoSuchElementException\n",
    "from selenium.webdriver.support.expected_conditions import NoSuchFrameException\n",
    "from selenium.webdriver.support.expected_conditions import StaleElementReferenceException\n",
    "from selenium.webdriver.support.expected_conditions import WebDriverException "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL STRUCTURE :\n",
    "\n",
    "# url = \"https://economictimes.indiatimes.com/archive/year-\" + str(year_name) + \",\" + \"month-\" + str(month_no) + \".cms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h1> SELENIUM VERISON - (COMPLETE SELENIUM) :<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each_year in years_range:\n",
    "    \n",
    "#     dict_record = {}\n",
    "    \n",
    "#     # find the total no of days in that\n",
    "    \n",
    "#     for each_month_no in months_range:\n",
    "        \n",
    "#         # frame url for this month of this year\n",
    "#         year_month_url = \"https://economictimes.indiatimes.com/archive/year-\" + str(each_year) + \",\" + \"month-\" + str(each_month_no) + \".cms\"\n",
    "#         print(year_month_url)\n",
    "        \n",
    "#         dict_record['year_month_url'] = year_month_url\n",
    "#         dict_record['year'] = each_year\n",
    "#         dict_record['month'] = each_month_no\n",
    "        \n",
    "#         # open the url for finding links of all dates present on this url\n",
    "#         responseFromYearMonthUrl = requests.get(year_month_url)\n",
    "#         soup = BeautifulSoup(responseFromYearMonthUrl.text, 'lxml')\n",
    "        \n",
    "#         # FIND ALL THE DATES LINKS TO ARCHIVES\n",
    "        \n",
    "#         all_dates_links_for_this_month_list = []\n",
    "#         all_dates_links_for_this_month = driver.find_elements_by_xpath(r'//table[@id=\"calender\"]//tr//a[contains(@href,\"/archivelist/year-\") and contains(@href,\"month-\") and contains(@href, \"starttime\")]')\n",
    "        \n",
    "#         for each_date_link in all_dates_links_for_this_month:\n",
    "#             all_dates_links_for_this_month_list.append(each_date_link.get_attribute('href'))\n",
    "        \n",
    "#         all_dates_links_for_this_month_list = set(all_dates_links_for_this_month_list)\n",
    "        \n",
    "#         for each_date_link in all_dates_links_for_this_month_list:\n",
    "            \n",
    "#             # go to that date link and it now displays all the archives for the day \n",
    "#             driver.get(each_date_link)\n",
    "            \n",
    "#             dict_temp['timestamp'] = driver.find_element_by_xpath(r'//*[@id=\"pageContent\"]/span/table[1]/tbody/tr[2]/td/b').text\n",
    "            \n",
    "#             all_articles_links_for_the_day_list = [] # all articles link will be ready in this list\n",
    "#             all_articles_links_for_the_day = driver.find_elements(By.XPATH, \"//td[@class='contentbox5']//ul//li//a\")\n",
    "            \n",
    "#             for each_article_link in all_articles_links_for_the_day:\n",
    "#                 all_articles_links_for_the_day_list.append(each_article_link.get_attribute('href'))\n",
    "            \n",
    "#             if len(all_articles_links_for_the_day_list) > 0:\n",
    "                \n",
    "#                 for each_article_link in all_articles_links_for_the_day_list:\n",
    "                    \n",
    "#                     # open this article url to see the article\n",
    "#                     driver.get(each_article_link)\n",
    "#                     sleep(5)\n",
    "\n",
    "#                     # Finally on the article page\n",
    "#                     # if 'Industry›Energy›Power'  or 'Industry›Energy›Oil & Gas' then scrape else move to next article webpage :\n",
    "\n",
    "#                     industry_check = driver.find_element_by_xpath(r\"//div[@class='clr breadCrumb']//span[2]\").text.strip()\n",
    "#                     energy_check = driver.find_element_by_xpath(r\"//div[@class='clr breadCrumb']//span[3]\").text.strip()\n",
    "#                     power_check = driver.find_element_by_xpath(r\"//div[@class='clr breadCrumb']//span[4]\").text.strip()\n",
    "#                     oilAndGas_check =driver.find_element_by_xpath(r\"//div[@class='clr breadCrumb']//span[4]\").text.strip()\n",
    "\n",
    "#                     if ((industry_check=='Industry' and energy_check=='Energy') and (power_check=='Power' or oilAndGas_check=='Oil & Gas')):\n",
    "#                         # scrape report\n",
    "#                         articleHeadline = driver.find_element(By.XPATH, \"//h1[@class='clearfix title']\").text.strip()\n",
    "#                         article_text = driver.find_element(By.XPATH, \"//div[@class='article_block gaDone'][1]//article[contains(@data-artidate,'')]//div[@class='artText']\").text\n",
    "#                         dict_temp['article heading'] = articleHeadline\n",
    "#                         dict_temp['article_text'] = article_text\n",
    "#                         print(\"FOUND -----------\")\n",
    "#                         print(dict_temp)\n",
    "#                         break\n",
    "\n",
    "\n",
    "#                     else:\n",
    "#                         # no evidence for energy>power or energy>oil & gas\n",
    "#                         print('NOT PRESENT------')\n",
    "#                         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h1> BEAUTIFULSOUP VERISON - (WHEREVER POSSIBLE) :<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo Collection - I  (for articles storage - MAIN COLLECTION):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db_EconomicTimes = client['db_EconomicTimes']\n",
    "collection_articles = db_EconomicTimes['collection_articles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo Collection- II (for keeping pointer/track of last day worked on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db_EconomicTimes = client['db_EconomicTimes']\n",
    "collection_last_day_flag = db_EconomicTimes['collection_last_day_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo Collection- II (for keeping pointer/track of last month covered):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db_EconomicTimes = client['db_EconomicTimes']\n",
    "collection_last_day_month_flag = db_EconomicTimes['collection_last_day_month_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllDatesLinks(page_source):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    On the page that shows the date links of that month inside a calender format, \n",
    "    find and store all the days links\n",
    "    '''\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    all_dates_links_for_this_month_list = []\n",
    "#     soup = BeautifulSoup(driver.page_source ,'lxml')\n",
    "    \n",
    "    # Target Table - calender\n",
    "    targetTable = soup.find('table', attrs={'id':'calender'})\n",
    "    \n",
    "#     return targetTable \n",
    "    sleep(2)\n",
    "    # all tds under the table - calender\n",
    "    tableRowDatas = targetTable.findAll('td')\n",
    "\n",
    "    for each in tableRowDatas:\n",
    "        \n",
    "        try:\n",
    "            date_link_a_tag = each.find('a',attrs={'href':re.compile(r\"/archivelist/year-\"), \n",
    "                                            'href':re.compile(r\"month-\"), \n",
    "                                            'href':re.compile(r\"starttime\")})\n",
    "\n",
    "\n",
    "            # If this is a date link then , as text, it should contain date no Ex <a tag>12</a tag>\n",
    "            if int(date_link_a_tag.text.strip()) in list(np.arange(1, 32)):  \n",
    "                all_dates_links_for_this_month_list.append('https://economictimes.indiatimes.com' + date_link_a_tag['href'])\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return list(set(all_dates_links_for_this_month_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getALLArchivesLinks(soup):\n",
    "    \n",
    "    '''\n",
    "    After opening a certain date link, we move to page displaying all archives of that day,,\n",
    "    hence we store all the link for that archives\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_articles_links_for_the_day_list = []\n",
    "\n",
    "    all_ul_tags = soup.findAll('ul', attrs={'class':'content'})  # max 2 uls\n",
    "    \n",
    "    for each_ul_tag in all_ul_tags:\n",
    "        \n",
    "        all_a_tags = each_ul_tag.findAll('a')\n",
    "        \n",
    "        for each_a_tag in all_a_tags:\n",
    "        \n",
    "            all_articles_links_for_the_day_list.append('https://economictimes.indiatimes.com' + each_a_tag['href'])\n",
    "    \n",
    "    return list(set(all_articles_links_for_the_day_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def checkLastTimeStamp():\n",
    "    \n",
    "#     each_element_time = {}\n",
    "    \n",
    "#     for each_element_time in collection_last_day_flag.find():\n",
    "#         continue\n",
    "    \n",
    "#     if len(each_element_time.keys()) == 0:\n",
    "#         return None\n",
    "    \n",
    "#     last_Latest_Timestamp_covered = each_element_time['last_Timestamp']\n",
    "    \n",
    "#     markerTimeStamp = pd.Timestamp(last_Latest_Timestamp_covered) \n",
    "#     return markerTimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN THE DRIVER INCOGNITO AND MAXIMIZE :\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--incognito')\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options, executable_path=r'C:\\Users\\shubhendra\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_year = pd.Timestamp('today').year\n",
    "latest_month = pd.Timestamp('today').month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_last_day_month_flag.delete_one({'year_month_url':'https://economictimes.indiatimes.com/archive/year-'+str(latest_year)+ ',month-'+ str(latest_month)+'.cms'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# markerTimeStamp = checkLastTimeStamp()\n",
    "    \n",
    "# if markerTimeStamp == None:\n",
    "#     print(\"RAW FRESH START\")\n",
    "#     years_range = np.arange(2019, 2000, -1)\n",
    "#     months_range = np.arange(12, 0, -1)\n",
    "\n",
    "# else:\n",
    "#     print(\"STARTING FROM YEAR : \", markerTimeStamp.year,)\n",
    "#     print(\"STARTING FROM MONTH : \", markerTimeStamp.month,)\n",
    "#     years_range = np.arange(markerTimeStamp.year,  2000, -1)\n",
    "#     months_range = np.arange(markerTimeStamp.month, 0, -1)\n",
    "\n",
    "\n",
    "latest_year = pd.Timestamp('today').year\n",
    "latest_month = pd.Timestamp('today').month\n",
    "\n",
    "collection_last_day_month_flag.delete_one({'year_month_url':'https://economictimes.indiatimes.com/archive/year-'+str(latest_year)+ ',month-'+ str(latest_month)+'.cms'})\n",
    "\n",
    "\n",
    "years_range = np.arange(pd.Timestamp('today').year,  2000, -1) \n",
    "months_range = np.arange(12, 0 , -1) \n",
    "\n",
    "for each_year in years_range:\n",
    "    \n",
    "    for each_month_no in months_range:\n",
    "        \n",
    "        # Frame url for this month of this year\n",
    "        year_month_url = \"https://economictimes.indiatimes.com/archive/year-\" + str(each_year) + \",\" + \"month-\" + str(each_month_no) + \".cms\"\n",
    "        \n",
    "                \n",
    "        if collection_last_day_month_flag.find_one({'year_month_url':str(year_month_url)}):\n",
    "            # MONTH ALREADY COVERED, HENCE MOVE TO NEXT MONTH\n",
    "            print('MONTH ALREADY COVERED')\n",
    "            continue\n",
    "        \n",
    "\n",
    "        ua = UserAgent()\n",
    "        header = {'User-Agent':ua.chrome}\n",
    "        responseYearMonthURL = requests.get(year_month_url, headers=header)\n",
    "        if responseYearMonthURL.status_code >=200 and responseYearMonthURL.status_code <=220:\n",
    "            \n",
    "            print(\"YEAR-MONTH URL EXISTS :-----------> \", year_month_url)\n",
    "\n",
    "            # OPENED the url for finding links of all dates present on this url inside calender \n",
    "\n",
    "            # Find all the day links for this month\n",
    "            # SELENIUM PAGESOURCE USED :\n",
    "            driver.get(year_month_url)\n",
    "            all_dates_links_for_this_month_list = getAllDatesLinks(driver.page_source)\n",
    "            \n",
    "            if len(all_dates_links_for_this_month_list) == 0 :\n",
    "                # IF THE MONTH CONSISTS OF NO DAYS I.E. NO CALENDER DISPLAYED\n",
    "                # IT MEANS WE ARE IN FUTURE MONTH \n",
    "                continue\n",
    "\n",
    "            for each_date_link in all_dates_links_for_this_month_list:\n",
    "\n",
    "                if collection_last_day_flag.find_one({'last_flag_day_link_to_AllArchives':each_date_link}):\n",
    "                    print('DAY ALREADY COVERED')\n",
    "                    # ALREADY COVERED ALL ACRHIVES FOR THIS DAY\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    # we need to cover Archives for this day\n",
    "                    # Go to that date link and it now displays all the archives for the day \n",
    "                    # SELENIUM USED:\n",
    "                    sleep(3)\n",
    "                    print(\"DAY LINK TO ALL ARCHIVES OF THE DAY : ----------> \", each_date_link)\n",
    "                    \n",
    "                    driver.get(each_date_link)\n",
    "                    sleep(4)\n",
    "                    driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "                    sleep(2)\n",
    "                    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "                    sleep(4)\n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "                    all_articles_links_for_the_day_list = getALLArchivesLinks(soup) # All articles link will be ready in this list\n",
    "                    \n",
    "\n",
    "                    # current date time year value as timestamp from all archives link page\n",
    "                    timestamp_day_month_year = pd.Timestamp(soup.find('td', attrs={'class':'contentbox5'}).findAll('b')[1].text)\n",
    "                    \n",
    "                    # CHECK WHETHER THERE ARE ANY \n",
    "                    if len(all_articles_links_for_the_day_list) > 0:\n",
    "\n",
    "                        for each_article_link in all_articles_links_for_the_day_list:\n",
    "                            \n",
    "                            if collection_articles.find_one({'article_url':str(each_article_link)}):\n",
    "                                # ARTICLE ALREADY SCRAPED AND PRESENT IN COLLECTION.ARTICLES\n",
    "                                # MOVE TO NEXT ARTICLE LINK\n",
    "                                continue\n",
    "\n",
    "                            if len(re.findall(r\"/industry/energy/\", str(each_article_link), re.IGNORECASE)) > 0:\n",
    "                                # when the article url mentions '/industry/energy/', then only hit request and scrape\n",
    "                                # else continue to next article url\n",
    "                                \n",
    "                                count_to_break_final_if_large_hitRequest_failure_occurs = 0\n",
    "                                \n",
    "                                while True: \n",
    "                                    # keep hitting requests till the article link response with code http 200 \n",
    "                                    sleep(5)\n",
    "                                    \n",
    "                                    try:\n",
    "                                        header = {'User-Agent':ua.chrome}\n",
    "                                        responseArticlePage = requests.get(each_article_link, headers=header, timeout=10)\n",
    "                                        \n",
    "                                        if responseArticlePage.status_code == 200 :\n",
    "                                            soup = BeautifulSoup(responseArticlePage.text, 'lxml')\n",
    "                                            break\n",
    "                                            \n",
    "                                        if responseArticlePage.status_code >= 400 : \n",
    "                                            # IF ARTICLE LINK LEADS TO 404 ERROR OR SOME OTHER ERROR TYPE, THEN BREAK\n",
    "                                            soup = BeautifulSoup(responseArticlePage.text, 'lxml')\n",
    "                                            break\n",
    "                                            \n",
    "                                    except:\n",
    "                                        print('Hitting request again to article page ')\n",
    "                                        count_to_break_final_if_large_hitRequest_failure_occurs += 1\n",
    "                                        \n",
    "                                        # WE WONT HIT REQUEST, MORE THAN 10 TIMES TO THE ARTICLE URL \n",
    "                                        if count_to_break_final_if_large_hitRequest_failure_occurs >= 10:\n",
    "                                            print('LIMIT TO HIT REQUEST EXCEEDED MORE THAN 10 TIMES')\n",
    "                                            break\n",
    "                                            \n",
    "                                        else:\n",
    "                                            continue\n",
    "                                \n",
    "                                if count_to_break_final_if_large_hitRequest_failure_occurs >= 10:\n",
    "                                    print('MOVING TO NEXT ARTICLE URL AS HITTING >=10, FAILED')\n",
    "                                    continue\n",
    "                                    \n",
    "                                \n",
    "                                try:\n",
    "                                    breadCrumbDivText = soup.find('div', attrs={'class':'breadCrumb'}).text \n",
    "                                except:\n",
    "                                    # ERROR 404 ON ARTICLE PAGE\n",
    "                                    print('ERROR 404 OCCURED, MOVING TO NEXT ARTICLE LINK')\n",
    "                                    continue\n",
    "                                    \n",
    "                                if len(re.findall(r\"Industry\", breadCrumbDivText) ) > 0:\n",
    "                                    if len(re.findall(r\"Energy\", breadCrumbDivText) ) > 0:\n",
    "                                        if len(re.findall(r\"Power|Oil & Gas\", breadCrumbDivText)) > 0:\n",
    "                                            \n",
    "                                            domain = None\n",
    "                                            if breadCrumbDivText.__contains__('Power'):\n",
    "                                                domain = 'Power'\n",
    "                                            \n",
    "                                            else:\n",
    "                                                domain = 'Oil & Gas'\n",
    "                                            \n",
    "                                            dict_record = {}\n",
    "                                            \n",
    "                                            # SCRAPE REPORT :\n",
    "                                            \n",
    "                                            try:\n",
    "                                                articleHeadline = soup.find('h1', attrs={'class':'clearfix title'}).text.strip()\n",
    "\n",
    "                                                # ARTICLE TEXT :\n",
    "                                                div_tag_article = soup.find('div', attrs={'class':'Normal'})\n",
    "                                                article_text = div_tag_article.text.strip()\n",
    "\n",
    "                                                dict_record['domain'] = domain\n",
    "                                                dict_record['article_heading'] = articleHeadline\n",
    "                                                dict_record['article_text'] = article_text\n",
    "                                                dict_record['article_url'] = each_article_link\n",
    "                                                dict_record['year_name'] = str(each_year)\n",
    "                                                dict_record['month_name'] = str(each_month_no)\n",
    "                                                dict_record['day_name'] = str(pd.Timestamp(timestamp_day_month_year).day)\n",
    "                                                dict_record['day_link_to_allArchives'] = each_date_link\n",
    "                                                dict_record['year_month_url'] = year_month_url\n",
    "                                                dict_record['timestamp'] = str(timestamp_day_month_year)\n",
    "                                            \n",
    "                                            except:\n",
    "                                                # SOME ERROR OCCURED WHILE FINDING THE DATA CONTENT ON THE ARTICLE URL PAGE, HENCE IGNORING THAT ARTICLE\n",
    "                                                # MOVE TO NEXT ARTICLE URL \n",
    "                                                print('ERROR ON FINDING DATA')\n",
    "                                                continue\n",
    "\n",
    "                                            print(\"FOUND/INSERTING ARTICLE ON - INDUSTRY/ENERGY :\")\n",
    "                                            collection_articles.insert_one(dict_record)\n",
    "#                                             print(dict_record)\n",
    "                        \n",
    "                        print(\"<------------------COVERED ALL THE ARTICLES FOR THE DAY-------------->\")\n",
    "                        \n",
    "                        # WHEN ALL ARTICLES FOR THE DAY ARE COVERED, THEN INSERT INTO FLAG COLLECTION :\n",
    "                        collection_last_day_flag.insert_one({'last_flag_day_link_to_AllArchives':str(each_date_link), \n",
    "                                                         'year_name':str(each_year), \n",
    "                                                         'month_name':str(each_month_no),\n",
    "                                                         'last_Timestamp':str(timestamp_day_month_year),\n",
    "                                                         'year_month_url':str(year_month_url)   \n",
    "                                                        })\n",
    "                    \n",
    "            # WHEN ALL DAYS FOR THE MONTH ARE NOT COVERED, THEN INSERT INTO MONTH FLAG COLLECTION :\n",
    "            total_days_of_this_month = pd.Timestamp(str(each_month_no) + ' ' + str(each_year)).days_in_month\n",
    "            if total_days_of_this_month != len(all_dates_links_for_this_month_list):\n",
    "                print(\"We are in the current latest month of the latest year\")\n",
    "                # if total no of days of this month of this year is not equal to the total days links found for this month for this year\n",
    "                # then dont insert this month into month_flag_collection\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # WHEN ALL DAYS FOR THE MONTH ARE COVERED, THEN INSERT INTO MONTH FLAG COLLECTION :\n",
    "            # I.E WHEN ALL THE ARCHIVES FROM ALL THE DAYS FOR THE MONTH HAVE BEEN SCRAPED \n",
    "            collection_last_day_month_flag.insert_one({'year_month_url':str(year_month_url)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.get(each_article_link)\n",
    "soup = BeautifulSoup(s.text, 'lxml').find('div', attrs={'class':'Normal'})\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE FOR A DAY - ALL ARCHIVES  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dates_links_for_this_month_list = getALLArchivesLinks(BeautifulSoup(driver.page_source, 'lxml'))\n",
    "\n",
    "# for each_date_link in all_dates_links_for_this_month_list:\n",
    "#     print(\"current_date link : \", each_date_link)\n",
    "    \n",
    "#     # Go to that date link and it now displays all the archives for the day \n",
    "#     # BEAUTIFULSOUP USED:\n",
    "#     responseAllArchivesForDayPage = requests.get('https://economictimes.indiatimes.com/'+ str(each_date_link), headers={'User-Agent':'Chrome'}, timeout=5)\n",
    "#     soup = BeautifulSoup(responseAllArchivesForDayPage.text, 'lxml')\n",
    "#     all_articles_links_for_the_day_list = getALLArchivesLinks(soup) # All articles link will be ready in this list\n",
    "\n",
    "\n",
    "#     if len(all_articles_links_for_the_day_list) > 0:\n",
    "#         print(\"No of total arrticles for the day :\",len(all_articles_links_for_the_day_list))\n",
    "        \n",
    "#         for each_article_link in all_articles_links_for_the_day_list:\n",
    "            \n",
    "#             if len(re.findall(r\"/Industry/Energy/\", str(each_article_link), re.IGNORECASE)) > 0:\n",
    "#                 # when the article url mentions 'enery' 'industry' ,then only hit requesta and scrape\n",
    "#                 # else continue to next article url\n",
    "            \n",
    "#             print(\"Current article url :\", each_article_link)\n",
    "#             # Open this article url to see the article\n",
    "#             # BEAUTIFULSOUP\n",
    "#             sleep(2)\n",
    "#             responseArticlePage = requests.get('https://economictimes.indiatimes.com/'+str(each_article_link), headers={'User-Agent':'Chrome'})\n",
    "#             soup = BeautifulSoup(responseArticlePage.text, 'lxml')\n",
    "\n",
    "#             # Finally on the article page\n",
    "#             # if 'Industry›Energy›Power'  or 'Industry›Energy›Oil & Gas' then scrape else move to next article webpage :\n",
    "            \n",
    "#             breadCrumbDivText = soup.find('div', attrs={'class':'breadCrumb'}).text \n",
    "            \n",
    "#             if len(re.findall(r\"Industry\", breadCrumbDivText) ) > 0:\n",
    "#                 if len(re.findall(r\"Energy\", breadCrumbDivText) ) > 0:\n",
    "#                     if len(re.findall(r\"Power|Oil & Gas\", breadCrumbDivText)) > 0:\n",
    "\n",
    "#                         # SCRAPE REPORT :\n",
    "#                         articleHeadline = soup.find('h1', attrs={'class':'clearfix title'}).text.strip()\n",
    "\n",
    "#                         # ARTICLE TEXT :\n",
    "#                         div_tag_article = soup.find('div', attrs={'class':'Normal'})\n",
    "#                         article_text = div_tag_article.text.strip()\n",
    "\n",
    "#                         print(articleHeadline, \"    \",  article_text)\n",
    "\n",
    "#                         print(\"FOUND -----------\")\n",
    "# #                         print(dict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
