{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from time import sleep\n",
    "\n",
    "import hashlib\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import chromedriver_binary\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import NoSuchElementException\n",
    "from selenium.webdriver.support.expected_conditions import NoAlertPresentException\n",
    "from selenium.webdriver.support.expected_conditions import NoSuchFrameException\n",
    "from selenium.webdriver.support.expected_conditions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONGO CONNECTION :\n",
    "\n",
    "client = MongoClient()\n",
    "db_DD_Forecast_II = client['db_DD_Forecast_II']\n",
    "# collect_city_evidence = db_DD_Forecast_II['collect_city_evidence']\n",
    "collect_city_evidence2 = db_DD_Forecast_II['collect_city_evidence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN THE DRIVER AND MAXIMIZE :\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--incognito')\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h1>I :</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CHECKING EVIDENCE OF CITIES  : -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleniumBasedActivityScroll(driver):\n",
    "    '''\n",
    "    for selenium based page, perform some scroll Activity\n",
    "    '''\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "    driver.execute_script('window.scrollBy(0, -100);')\n",
    "    driver.execute_script('window.scrollBy(0, 50);')\n",
    "    driver.execute_script('window.scrollBy(0, -50);')\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goBack(driver):\n",
    "    \n",
    "    while (True):\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "        sleep(4)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        if len(re.findall(r\"Find historical weather by searching for a city, zip code, or airport code. Include a date for which you would like to see weather history. You can select a range of dates in the results on the next page\", str(soup))) > 0:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR EACH CITY, FIND THE EVIDENCE OF DATA TABLE EXISTENCE :\n",
    "\n",
    "driver.get(\"https://www.wunderground.com/history/\")\n",
    "\n",
    "for i in range(0, total_cities):  # iterate through each city provided externally\n",
    "    \n",
    "    dict_evidence = {}\n",
    "    \n",
    "    each_city_india = listOfCities.loc[i, 'name'] # city to process (from csv)\n",
    "    state_corresponding = listOfCities.loc[i, 'name_1']  # state of this city (from csv)\n",
    "        \n",
    "        \n",
    "    if collect_city_evidence2.find_one({\"city_from_csv\":each_city_india.strip(), \"state_from_csv\":state_corresponding}):\n",
    "        print(\"city already covered in the collection\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Open the driver \n",
    "    print(\"working for :\",\"city from csv :\", each_city_india, \" state from csv :\", state_corresponding, \"opening url\")\n",
    "    \n",
    "    sleep(5)\n",
    "#     driver.get(\"https://www.wunderground.com/history/\")\n",
    "    driver.implicitly_wait(10) \n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if len(re.findall(r\"Error 404: Page Not Found\", str(soup))) > 0 and len(re.findall(r\"Either Brutus is in the server room or it might be that\", str(soup))) > 0:\n",
    "        sleep(1800) # sleep for 1/5 hours\n",
    "        \n",
    "        print(\"************************CLOSING*****************************\")   \n",
    "        #  driver.close()\n",
    "        driver.quit()\n",
    "\n",
    "        \n",
    "    \n",
    "    # else start exploring for this city \n",
    "    evidence_city_flag = 0\n",
    "    \n",
    "    each_city = each_city_india.strip()\n",
    "    each_city_search_input = each_city + \", \" + \"india\"  # input term to location search bar \n",
    "    \n",
    "    # Set the year value to '2018':\n",
    "    year_select = driver.find_element(By.XPATH, \"//select[@class='year']//option[contains(text(),'2018')]\")\n",
    "    year_select.click()\n",
    "    \n",
    "#     # Set the month value to 'january':\n",
    "#     month_select = driver.find_element(By.XPATH, \"//select[@class='month']//option[contains(text(),'anuary')]\")\n",
    "#     month_select.click()\n",
    "\n",
    "    \n",
    "    location_input_bar = driver.find_element(By.XPATH, \"//input[@placeholder='City, State or ZIP Code, or Airport Code' and @id='histSearch']\")\n",
    "    location_input_bar.clear()\n",
    "    sleep(4)\n",
    "    location_input_bar.send_keys(each_city_search_input)  # made an input of 'city name' to location bar\n",
    "    sleep(9)\n",
    "    location_input_bar.send_keys(Keys.DOWN)  # press down key only to select from the 1st option shown by site\n",
    "    \n",
    "    \n",
    "    # WE need to verify whether the option provided by site contains term 'India' or not :    \n",
    "    \n",
    "    # prevent element staleness\n",
    "    location_input_bar = driver.find_element(By.XPATH, \"//input[@placeholder='City, State or ZIP Code, or Airport Code' and @id='histSearch']\")\n",
    "    proper_search_term_site_gives = location_input_bar.get_attribute('value')\n",
    "    \n",
    "    \n",
    "    # Noting in dictionary :\n",
    "    # 1.) test month\n",
    "    # 2.) test day\n",
    "    # 3.) test year\n",
    "    \n",
    "    dict_evidence['test month'] = driver.find_element(By.XPATH, r\"//select[@class='month']//option[@selected='selected']\").text\n",
    "    dict_evidence['test day'] = driver.find_element(By.XPATH, r\"//select[@class='day']//option[@selected='']\").text\n",
    "    dict_evidence['test year']= driver.find_element(By.XPATH, r\"//select[@class='year']//option[contains(text(),'2018')]\").text\n",
    "\n",
    "    \n",
    "    if len(re.findall(r\"India\", str(proper_search_term_site_gives))) > 0:\n",
    "        # Contains 'India' term in input location search bar\n",
    "        \n",
    "                \n",
    "        # Submit the information:\n",
    "        submit_button = driver.find_element(By.XPATH, \"//input[@type='submit' and @class='button radius' and @ value='Submit']\")\n",
    "        \n",
    "        # AFTER SUBMIT BUTTON CLICKED, SEVERAL OPTIONS POSSIBLE :----\n",
    "        submit_button.click()\n",
    "        \n",
    "        sleep(7) # let this city webpage open and load\n",
    "        \n",
    "        \n",
    "        # CASE_A : AFTER 'SUBMIT' BUTTON CLICK, THE PAGE DOES NOT LOADS TO A NEW PAGE, INSTEAD ASKS FOR SELECT LOCATION :\n",
    "        if str(driver.current_url).__contains__(r\"https://www.wunderground.com/history/index.html?error=AMBIGUOUS\"):\n",
    "            select_a_location_text_existence = driver.find_element(By.XPATH, r\"//div[@class='location-date-column']//p[@class='listHeading']\").text \n",
    "            \n",
    "            if len(re.findall(r\"Select a location\", select_a_location_text_existence)) > 0:\n",
    "                \n",
    "                under_select_a_location_names = driver.find_elements(By.XPATH, \"//li//a[text()=\"  + \"'\" + proper_search_term_site_gives + \"'\"  +  \"]\" )\n",
    "                \n",
    "                # Ex. ['Rupnagar, India', 'Rupnagar, India']\n",
    "                names_list = [each_name.text for each_name in under_select_a_location_names] \n",
    "                \n",
    "                count_names = 0\n",
    "                \n",
    "                for each_name_in_list_names in names_list:\n",
    "                    if proper_search_term_site_gives == each_name_in_list_names:\n",
    "                        count_names += 1\n",
    "                        \n",
    "                if count_names == len(names_list):  \n",
    "                    # CASE_A PROVEN TRUE, HENCE STORING THE LINKS :\n",
    "                    \n",
    "                    under_select_a_location_links = driver.find_elements(By.XPATH, \"//li//a[text()=\"  + \"'\" + proper_search_term_site_gives + \"'\"  +  \"]\" )\n",
    "                    links_list = [each_link_location.get_attribute('href') for each_link_location in under_select_a_location_links]\n",
    "                    # NOT CHECKING EVIDENCE FOR DATA TIME BEING NOW, JUST STORING THE LINKS FOR CITY DATA PAGE\n",
    "                    \n",
    "                    dict_evidence['site_option_for_city_csv'] = proper_search_term_site_gives\n",
    "                    dict_evidence['city url'] = links_list\n",
    "                    dict_evidence['specical case'] = 1\n",
    "                    dict_evidence['city_from_csv'] = each_city\n",
    "                    dict_evidence['state_from_csv'] = state_corresponding\n",
    "                    dict_evidence['evidence_city'] = 1\n",
    "                    collect_city_evidence2.insert_one(dict_evidence)\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        # CHECK FOR SERVER ERROR 404 :\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        if len(re.findall(r\"Error 404: Page Not Found\", str(soup))) > 0 and len(re.findall(r\"Either Brutus is in the server room or it might be that\", str(soup))) > 0:\n",
    "            dict_evidence['site_option_for_city_csv'] = proper_search_term_site_gives\n",
    "            dict_evidence['evidence_city'] = 0\n",
    "            dict_evidence['city_from_csv'] = each_city\n",
    "            dict_evidence['state_from_csv'] = state_corresponding\n",
    "            dict_evidence['city url'] = driver.current_url\n",
    "            dict_evidence['Error 404 '] = 1  # error 404 brutus in server room\n",
    "            collect_city_evidence2.insert_one(dict_evidence)\n",
    "            goBack(driver)\n",
    "            sleep(5)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # SEARCH PATTERN 1 FOR BS4 BASED DATA HEADING (FROM BS4 PERSPECTIVE):\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            sleep(2.5)\n",
    "            \n",
    "            dict_evidence['heading'] = re.sub(r'[\\n\\t]', '', soup.find('div', attrs={'class':'mainWrapper'}).find('div', attrs={'id':'location'}).h2.text).strip()\n",
    "#             search_pattern_1 = soup.find('div', attrs={\"id\":\"observations_details\"}).table\n",
    "            evidence_city_flag = 1\n",
    "            dict_evidence['BS4 based'] = 1\n",
    "#             dict_evidence['heading'] = driver.find_element(By.XPATH, \"//div[@class='mainWrapper']//div[@id='location']//h2[1]\").text.strip()\n",
    "        except:\n",
    "            # FROM BS4 PERSPECTIVE, HEADING WAS NOT FOUND, SO SEARCHING FOR HEADING FROM SELENIUM PERSPECTIVE :\n",
    "            \n",
    "            try:\n",
    "                # SEARCH PATTERN 2 FOR SELENIUM BASED DATA HEADING (FROM SELENIUM PERSPECTIVE):\n",
    "                sleep(8)          \n",
    "\n",
    "                seleniumBasedActivityScroll(driver) # perform selenium based scroll activity\n",
    "                driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "                seleniumBasedActivityScroll(driver) # perform selenium based scroll activity twice\n",
    "                driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "                seleniumBasedActivityScroll(driver) # perform selenium based scroll activity twice\n",
    "                sleep(5)\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                \n",
    "                dict_evidence['heading'] = re.sub(r'[\\n\\t]','', soup.find('div', attrs={'class':'columns small-12'}).find('h1').text).strip()\n",
    "#                 search_pattern_2 = soup.find('div', attrs={\"class\":\"observation-table\"}).table\n",
    "                evidence_city_flag = 1\n",
    "                dict_evidence['selenium based'] = 1 # requires selenium\n",
    "#                 dict_evidence['heading'] = driver.find_element(By.XPATH, '//div[@class=\"columns small-12\"]//h1').text.strip()\n",
    "\n",
    "            except:\n",
    "#                 # FROM SELENIUM PERSPECTIVE, DATA TABLE WAS NOT FOUND, SO SEARCHING FOR HEADING IF AVAILABLE\n",
    "\n",
    "#                 try:\n",
    "#                     dict_evidence['heading'] = driver.find_element(By.XPATH, '//div[@class=\"columns small-12\"]//h1').text.strip()\n",
    "#                     evidence_city_flag = 0\n",
    "#                 except:\n",
    "\n",
    "                # NEITHER BS4 (TABLE+HEADING) WORKED NOR SELENIUM (TABLE+HEADING) WORKED, \n",
    "                # no bs4 table and no bs4 heading\n",
    "                # no selenium table and no selenium heading\n",
    "                # completely broken case\n",
    "                evidence_city_flag = 0\n",
    "\n",
    "                        \n",
    "        dict_evidence['site_option_for_city_csv'] = proper_search_term_site_gives\n",
    "        dict_evidence['evidence_city'] = evidence_city_flag\n",
    "        dict_evidence['city_from_csv'] = each_city\n",
    "        dict_evidence['state_from_csv'] = state_corresponding\n",
    "        dict_evidence['city url'] = driver.current_url\n",
    "        \n",
    "        # into the collection insert\n",
    "        print(\"inserting.....\")\n",
    "        collect_city_evidence2.insert_one(dict_evidence)\n",
    "        \n",
    "        goBack(driver)\n",
    "                        \n",
    "        sleep(5)\n",
    "        print('evidence is ', evidence_city_flag)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # 'city name and location' search term that site gave didn't contain 'India'. \n",
    "        #  Conclusion: 1.) site rounded off to something else for our city input, maybe out of India OR \n",
    "        #              2.) site didn't show any options for our city from csv, hence might not contain city data at all.\n",
    "        \n",
    "#         evidence_city_flag = 0\n",
    "        dict_evidence['site_option_for_city_csv'] = 'not_generated' # site dint suggest any options\n",
    "        dict_evidence['evidence_city'] = 0\n",
    "        dict_evidence['city_from_csv'] = each_city\n",
    "        dict_evidence['state_from_csv'] = state_corresponding\n",
    "        \n",
    "        # into the collection insert\n",
    "        collect_city_evidence2.insert_one(dict_evidence)\n",
    "        \n",
    "        \n",
    "        # search term to location bar, site gave\n",
    "        # each_city\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table targets we need to find as evidence :\n",
    "\n",
    "# 1.) div_table_target = soup.find('div', attrs={\"id\":\"observations_details\"}).table  # BS4 BASED\n",
    "# 2.) div_table_target = soup.find('div', attrs={\"class\":\"observation-table\"}).table  # SELENIUM BASED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h1>II :</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *<h1>SPECICAL CASES :*</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">+ SPECIAL CASES :\n",
    "\n",
    ">+ WILL BE STORED IN DIFFERENT COLLECTION UNDER THE SAME DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONGO CONNECTION :\n",
    "\n",
    "client = MongoClient()\n",
    "db_DD_Forecast_II = client['db_DD_Forecast_II']\n",
    "collect_city_evidence2_specicalCases = db_DD_Forecast_II['collect_city_evidence2_specicalCases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for each_specical_case in collect_city_evidence2.find({'specical case':1}):\n",
    "    \n",
    "    dict_special_case = {}\n",
    "    \n",
    "    # These parameters are set already\n",
    "    \n",
    "    if collect_city_evidence2_specicalCases.find_one({\"city_from_csv\" : each_specical_case['city_from_csv'] , 'state_from_csv':each_specical_case['state_from_csv']}):\n",
    "        # ALREADY COVERED THAT CITY FROM CSV\n",
    "        print('city from csv already covered')\n",
    "        continue\n",
    "     \n",
    "    dict_special_case['test month'] = each_specical_case['test month'] \n",
    "    dict_special_case['test day'] = each_specical_case['test day'] \n",
    "    dict_special_case['test year'] = each_specical_case['test year'] \n",
    "    dict_special_case['site_option_for_city_csv'] = each_specical_case['site_option_for_city_csv'] \n",
    "    dict_special_case['city_from_csv'] = each_specical_case['city_from_csv']\n",
    "    dict_special_case['state_from_csv'] = each_specical_case['state_from_csv']\n",
    "    dict_special_case['city url'] = each_specical_case['city url'] \n",
    "    \n",
    "    \n",
    "    for count, each_url in enumerate(each_specical_case['city url']):\n",
    "        \n",
    "        \n",
    "        dict_special_case['city_url_' + str(count)] = each_url\n",
    "                \n",
    "        # REQUEST THIS URL\n",
    "        ua = UserAgent()\n",
    "        header = {'User-Agent':str(ua.chrome)}\n",
    "        responseCityUrl = requests.get(each_url, headers=header)\n",
    "        sleep(4)\n",
    "        soup = BeautifulSoup(responseCityUrl.text, 'lxml')\n",
    "        sleep(2)\n",
    "        \n",
    "        \n",
    "        # CHECK FOR SERVER ERROR 404 :\n",
    "        if len(re.findall(r\"Error 404: Page Not Found\", str(soup))) > 0 and len(re.findall(r\"Either Brutus is in the server room or it might be that\", str(soup))) > 0:\n",
    "            dict_special_case['Error 404_' + str(count)] = 1\n",
    "            dict_special_case['BS4 based_' + str(count)] = 0\n",
    "            dict_special_case['selenium based_' + str(count)] = 0 \n",
    "            continue\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            # SEARCHING FOR BS4 BASED HEADING\n",
    "            # IF BS4 BASED HEADING IS FOUND, THEN THE DATA EXISTS\n",
    "            \n",
    "            dict_special_case['heading_' + str(count)] = re.sub(r'[\\n\\t]', '', soup.find('div', attrs={'class':'mainWrapper'}).find('div', attrs={'id':'location'}).h2.text).strip()\n",
    "            dict_special_case['BS4 based_' + str(count)] = 1\n",
    "            dict_special_case['selenium based_' + str(count)] = 0\n",
    "            dict_special_case['evidence_city_' + str(count)] = 1\n",
    "            \n",
    "        except:\n",
    "            # BS4 BASED HEADING WAS NOT FOUND , \n",
    "            # SO SEARCHING FOR SELENIUM BASED HEADING \n",
    "            \n",
    "            try:\n",
    "                dict_special_case['heading_' + str(count)] = re.sub(r'[\\n\\t]','', soup.find('div', attrs={'class':'columns small-12'}).find('h1').text).strip()\n",
    "                dict_special_case['BS4 based_' + str(count)] = 0\n",
    "                dict_special_case['selenium based_' + str(count)] = 1\n",
    "                dict_special_case['evidence_city_' + str(count)] = 1\n",
    "                \n",
    "            except:\n",
    "                # NEITHER BS4 BASED NOR SELENIUM BASED HEADING WAS FOUND\n",
    "                # HENCE POSSIBILITIES ARE \n",
    "                # 1.) ERROR 404 \n",
    "                # 2.) SITE DIDN'T RESPOND VIA REQUESTS MODULE PYTHON\n",
    "                \n",
    "                dict_special_case['BS4 based_' + str(count)] = 0\n",
    "                dict_special_case['selenium based_' + str(count)] = 0\n",
    "                dict_special_case['evidence_city_' + str(count)] = 0\n",
    "                dict_special_case['unknown error_' + str(count)] = 1\n",
    "\n",
    "                \n",
    "        # INSERTING INTO A SEPARATE COLLECTION NAMED 'collect_city_evidence2_specicalCases' : \n",
    "        \n",
    "#         uniqueStringToEncode = each_specical_case['city_from_csv'] + each_specical_case['state_from_csv'] \n",
    "#         mongoIdHash = hashlib.md5(uniqueStringToEncode.encode('utf-8')).hexdigest()        \n",
    "        \n",
    "        \n",
    "#         dict_special_case['_id'] = mongoIdHash\n",
    "        \n",
    "    collect_city_evidence2_specicalCases.insert_one(dict_special_case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
